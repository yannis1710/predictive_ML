{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. About\n",
    "\n",
    "The purpose of this assigment is to explore and evaluate the capability of variaous machine learning algorithms in predicting which fish species you catch given different features such as gear used, geographical locations, boat size, and sea depth. We find it interesting to find out whether or not all types of gears are necessary. For instance, environmental consequences of bottom trawling; disturbance of seabed ecosystems.\n",
    "\n",
    "Source: https://www.hi.no/hi/nyheter/2023/januar/traling-pavirker-livet-pa-havbunnen\n",
    "\n",
    "A possible outcome of the assignment is to investigate if other techniques can get the same amount and species of fish. \n",
    "\n",
    "\n",
    "\n",
    "The dataset used in this assignment consits of a collection of fishing records, which among others include the gear, locations, boat sizes, and sea depths. Each record in the dataset also includes the species of fish caught during each trip. \n",
    "\n",
    "In our assignment, we will use Naïve Bayes and Random Forest to set a baseline for comparison. Furthermore, we will create a deep learning model, using neural networks, and experiment with an unsupervised learning model. The performance of the models will be evaluated based on their accuracy and precision in predicting the correct fish species. \n",
    "\n",
    "An important part of this assignment is feature selection and engineering, where we will attempt to identify which factors have the most significant influence on the prediction of fish species.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans \n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime \n",
    "\n",
    "\n",
    "# Unsupervised\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "\n",
    "# path that everyone can use \n",
    "workdir = os.getcwd()\n",
    "parent = os.path.dirname(workdir)\n",
    "\n",
    "file_path = 'elektronisk-rapportering-ers-2018-fangstmelding-dca-simple.csv'\n",
    "full_path = os.path.join(parent, file_path)\n",
    "df = pd.read_csv(full_path, sep=\";\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PRESENTATION OF DATA \n",
    "The data is distributed from Fiskedirektoratet from 2017-2019. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45 columns total. \n",
    "\n",
    "27 columns with 'object' values, which we later have to change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the first five samples with five random ones. 'Bruttonnasje 1969' and 'Bruttotonnasje annen' have alternating NaN values. Some columns have commas; others have periods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. INSPECTION OF DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object gives the count, unique entries in the columns, the most common value in each column, and the most common value's frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Havdybde start' and 'Havdybde stopp' contain positive values - counterintuitive as sea depth should be negative.\n",
    "- 'Fangstår' max is 2018 - should be 2019. \n",
    "- 'Varighet' - large max value (125534)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df has 305,434 and 45 columns - useful for comparing after rows and columns are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "_________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PREPROCESSING DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Check for and remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reviewing the dataset, we noted potential duplicy and decided to remove duplicates to reduce redundancy, and increase accuracy and efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = df.shape[0]\n",
    "deleted_samples = []\n",
    "\n",
    "for sample_num in range(0,num_of_samples-1): \n",
    "    row = df.iloc[sample_num]\n",
    "    next_row = df.iloc[sample_num + 1]\n",
    "    # when the samples are identical they will be appended to the empty list\n",
    "    append = True \n",
    "    # a loop for all samples that are unique\n",
    "    for feature_values in zip(row, next_row): \n",
    "        if feature_values[0] != feature_values[1]: # if they are not identical, the next row is checked\n",
    "            if pd.isna(feature_values[0]) and pd.isna(feature_values[1]):\n",
    "                continue \n",
    "            append = False\n",
    "            break \n",
    "    if append == True: \n",
    "        deleted_samples.append(sample_num)\n",
    " \n",
    "df.drop(df.index[deleted_samples], inplace=True) # dropping all duplicate rows\n",
    "print(deleted_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are nine identical rows. Given the small number of duplicates, it is highly unlikely that their removal will significantly impact the models performance.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Remove columns\n",
    "\n",
    "\n",
    "Dropping colums that will not be used simplifies the dataset, and focuses the analysis on important factors. Columns used in preprocessing will be dropped later. \n",
    "\n",
    "\n",
    "- We keep 'Startdato', and remove other columns associated with time. \n",
    "- We keep key columns like 'Havdybde start', 'Havdybde stopp', and positional coordinates, and remove redundant ones like 'Hovedområde' and 'Lokasjon'. \n",
    "- We keep 'Redskap FDIR', and remove other columns associated with gear. \n",
    "- We keep 'Hovedart FAO', and remove other columns associated with species. \n",
    "- We remove 'Fartøylengde' and 'Bredde' - and keep gross tonnage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Meldingstidspunkt', 'Melding ID', 'Meldingsdato', 'Meldingsklokkeslett', 'Starttidspunkt', 'Startklokkeslett', 'Stopptidspunkt', 'Stoppdato', 'Stoppklokkeslett', 'Varighet', 'Fangstår', 'Hovedområde start (kode)', 'Hovedområde stopp (kode)', 'Hovedområde stopp', 'Lokasjon stopp (kode)', 'Redskap FAO (kode)', 'Redskap FAO', 'Redskap FDIR (kode)', 'Hovedart FAO (kode)', 'Hovedart - FDIR (kode)', 'Art FAO', 'Art FAO (kode)', 'Art - FDIR (kode)', 'Art - FDIR', 'Art - gruppe (kode)', 'Art - gruppe', 'Rundvekt', 'Lengdegruppe (kode)', 'Lengdegruppe', 'Fartøylengde', 'Bredde'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. NaN values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summing NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of NaN values (except for Bruttotonnasje) are a few.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'Bruttotonnasje' NaN\n",
    "\n",
    "We chose 'Bruttotonnasje' to measure vessels because it indicates internal volume and storage capacity, while 'Bredde' and 'Fartøylengde' only measure surface area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Brutotonnasje 1969' and 'Bruttotonnasje annen' have about 300,000 NaN values combined. To preserve data, we replace these with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bruttotonnasje 1969'] = df['Bruttotonnasje 1969'].fillna(0)\n",
    "df['Bruttotonnasje annen'] = df['Bruttotonnasje annen'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df has been reduced to 13 columns and 10,000 fewer samples, which is a reasonable redution for the size of our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Converting columns to numerical format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful because we will later use these columns in mathematical operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_convert = ['Bruttotonnasje 1969', 'Bruttotonnasje annen', 'Havdybde start', 'Havdybde stopp', 'Startposisjon bredde', 'Startposisjon lengde', 'Stopposisjon lengde', 'Stopposisjon bredde']\n",
    "\n",
    "for kolonne in features_convert: \n",
    "    df[kolonne] = df[kolonne].astype(str) # convert columns into string\n",
    "    df[kolonne] = df[kolonne].str.replace(',', '.').astype(float) #replace comma with period, and convert back to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. INSPECTION AND VISUALIZATION OF SELECTED FEATUERS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. 'Hovedart FAO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_counts = df['Hovedart FAO'].value_counts(sort=True) # sorting in descending order \n",
    "for species, count in species_counts.items():\n",
    "    print(f\"'{species}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64 unique species. Several are caught only a few times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of the top 20 species\n",
    "top_20 = species_counts[:20]\n",
    "top_species = top_20.index.tolist()\n",
    "top_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_of_species = species_counts[20:] \n",
    "species_to_remove = rest_of_species.index.tolist()\n",
    "\n",
    "df = df.loc[df['Hovedart FAO'].isin(species_to_remove) == False] # removing all species that are not in top 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. 'Havdybde'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df['Havdybde start'] > 0).sum())\n",
    "print((df['Havdybde stopp'] > 0).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1053 values in 'Havdybde start' and 1185 in 'Havdybde stopp' are positive. \n",
    "\n",
    "These numbers are small, but also irregularities that have to be addressed later. We want to ensure accuracy of our data, and avoid skewing the model with inaccurate information.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual summary of sea depth will help understanding the range. \n",
    "\n",
    "Plotting 'Havdybde start' and 'Havdybde stopp' together because we want a quick summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beregn antall forekomster av hver unik verdi\n",
    "start_counts = df['Havdybde start'].value_counts().sort_index() \n",
    "stopp_counts = df['Havdybde stopp'].value_counts().sort_index()\n",
    "\n",
    "plt.plot(start_counts, label='Havdybde start') # Plotting frequency \n",
    "plt.plot(stopp_counts, label='Havdybde stopp')\n",
    "\n",
    "plt.xlabel(\"Havdybde verdi\")\n",
    "plt.ylabel(\"Antall forekomster\")\n",
    "plt.title(\"Frekvens av 'Havdybde start' og 'Havdybde stopp'\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key observations:\n",
    "\n",
    "- Several sea depths occur few times, suggesting specialized or uncommon fishing zones. \n",
    "- Confirming positive values.\n",
    "\n",
    "https://pandas.pydata.org/docs/getting_started/intro_tutorials/04_plotting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all positive sea depths into negative values maintains integrity of the dataset, and ensures accurate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "havdybde_start_max = df['Havdybde start'].max()\n",
    "havdybde_stopp_max = df['Havdybde stopp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Havdybde start'] >= 0) & (df['Havdybde start'] <= havdybde_start_max), 'Havdybde start'] = -df['Havdybde start']\n",
    "df.loc[(df['Havdybde stopp'] >= 0) & (df['Havdybde stopp'] <= havdybde_stopp_max), 'Havdybde stopp'] = -df['Havdybde stopp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing a new feature calculated as the mean sea depths, will simplify the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Havdybde avg'] = df[['Havdybde start', 'Havdybde stopp']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Havdybde avg'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key observations: \n",
    "\n",
    "- 25% of the data falls below -274 meters.\n",
    "- mean depths is -225.77 meters.\n",
    "- std is 195 meters, suggesting a considerable variation in depths. \n",
    "\n",
    "Using std and mean depth (-225.77+195, -225.77+195) to get a number to start with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = df['Hovedart FAO'].count()\n",
    "samples_in_range = df[(df['Havdybde avg'] >= -450) & (df['Havdybde avg'] <= -30)].count()\n",
    "percentage_in_range = (samples_in_range['Hovedart FAO'] / total_samples) * 100\n",
    "print(percentage_in_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% of the fishing occurs between -30 and -450 meters, reasonable amount to keep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting 'Havdybde avg' so we can evaluate 'Havdybde avg'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_counts = df['Havdybde avg'].value_counts().sort_index() # Calculates the number of occurrences of each unique value.\n",
    "\n",
    "plt.plot(start_counts, label='Havdybde avg') # Plotting frequencies \n",
    "plt.xlabel(\"Havdybde verdi\")\n",
    "plt.ylabel(\"Antall forekomster\")\n",
    "plt.title(\"Frekvens av 'Havdybde avg'\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key observations: \n",
    "\n",
    "- Positive values have been eliminated. \n",
    "- Still a wide range of depths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find it fitting to keep sea depths between -30 and -450 meters, as this contains the most relevant information for our task, and will ensure more precision and reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Havdybde avg\"]= df[\"Havdybde avg\"].clip(upper = -30) \n",
    "df = df[df['Havdybde avg'] > -450]\n",
    "df['Havdybde avg'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that the top species are within the specified depth range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "havdybde_df = df[['Havdybde start', 'Havdybde stopp', 'Hovedart FAO']].copy()\n",
    "\n",
    "havdybde_filtered = havdybde_df[havdybde_df['Hovedart FAO'].isin(top_species)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_ranges = havdybde_filtered.groupby('Hovedart FAO')['Havdybde start'].agg(['min', 'max']).reset_index() # Find the depth ranges for each top species \n",
    "\n",
    "print(depth_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_counts = df['Havdybde avg'].value_counts().sort_index()  # Calculates the number of occurrences of each unique value.\n",
    "\n",
    "plt.plot(start_counts, label='Havdybde avg') # Plotting frequencies \n",
    "plt.xlabel(\"Havdybde avg verdi\")\n",
    "plt.ylabel(\"Antall forekomster\")\n",
    "plt.title(\"Frekvens av 'Havdybde avg'\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows us that the values are distributed more evenly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. 'Redskap FDIR'\n",
    "\n",
    "Gear is an important feature to inspect. \n",
    "\n",
    "Different gear types are tailored to specific species by habitat and size, with some optimized for bulk catching and others for selective targeting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equipment = []\n",
    "columns = list(df['Redskap FDIR'])\n",
    "for equipment in columns: \n",
    "    if equipment not in all_equipment: \n",
    "        all_equipment.append(equipment) #appends all unique gear types to empty list\n",
    "\n",
    "for item in all_equipment:\n",
    "    print(item)\n",
    "\n",
    "print(len(all_equipment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 18 unique gear types (including one NaN). This helps create a basis for sorting gear, and find out what gear typically catches different species. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These groups are sorted based on where they generally fish, and each group catches the same species. \n",
    "\n",
    "https://www.fiskeridir.no/Yrkesfiske/Tema/redskapshefte/Redskapshefte.pdf\n",
    "\n",
    "Four gears are removed because these are used less than 500 times, to make it easier for the models later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redskap_top = ['Flytetrål', 'Dobbeltrål', 'Snurpenot/ringnot', 'Flytetrål par']\n",
    "redskap_bottom = ['Bunntrål', 'Snurrevad', 'Teiner', 'Bunntrål par', 'Reketrål']\n",
    "redskap_settegarn = ['Settegarn']\n",
    "redskap_udef_garn = ['Udefinert garn', 'Andre liner']\n",
    "redskap_udef_trål = ['Udefinert trål']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vessel_group will be applied to gear in the feature engineering function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to categorize gear into one of five groups\n",
    "def vessel_group(equipment, group): \n",
    "     # if gear is present in any of the predefined lists, and found in corresponding group list, return 1\n",
    "    if group == 1 and equipment in redskap_top: \n",
    "         return 1 \n",
    "    elif group == 2 and equipment in redskap_bottom: \n",
    "         return 1 \n",
    "    elif group == 3 and equipment in redskap_settegarn: \n",
    "         return 1 \n",
    "    elif group == 4 and equipment in redskap_udef_garn: \n",
    "         return 1 \n",
    "    elif group == 5 and equipment in redskap_udef_trål: \n",
    "         return 1 \n",
    "    else: \n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the gear may lead to inconsistencies and biases. Each gear may catch more than one species, and if each gear is typically used in a particular area, it might affect which species we expect to catch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. 'Startdato' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Startdato'] = pd.to_datetime(df['Startdato'], format='%d.%m.%Y') # parse data to get date format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Month'] = df['Startdato'].dt.month # extract month \n",
    "df['Month'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that determines the season of given dates based on month and day\n",
    "def get_season(date: datetime, north_hemisphere: bool = True) -> str: \n",
    "    now = (date.month, date.day)\n",
    "    if (3, 21) <= now < (6, 21):\n",
    "        season = 'Spring' if north_hemisphere else 'Fall'\n",
    "    elif (6, 21) <= now < (9, 21):\n",
    "        season = 'Summer' if north_hemisphere else 'Winter'\n",
    "    elif (9, 21) <= now < (12, 21):\n",
    "        season = 'Fall' if north_hemisphere else 'Spring'\n",
    "    else:\n",
    "        season = 'Winter' if north_hemisphere else 'Summer'\n",
    "\n",
    "    return season\n",
    "\n",
    "df['Season'] = df['Startdato'].apply(get_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/16139306/determine-season-given-timestamp-in-python-using-datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_dummies = pd.get_dummies(df['Season'])\n",
    "df = pd.concat([df, season_dummies], axis=1) # concatenates dummies as columns in df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Trekkavstand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Trekkavstand'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large difference between max value and top 75%, suggesting outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trekk_counts = df['Trekkavstand'].value_counts()\n",
    "print(trekk_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28948 unique values in 'Trekkavstand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn histogram\n",
    "sns.set(style='whitegrid')\n",
    "sns.histplot(df['Trekkavstand'], bins=50, kde=False)  # kde=False removes estimated density curve\n",
    "plt.title('Histogram av Trekkavstand')\n",
    "plt.xlabel('Trekkavstand')\n",
    "plt.ylabel('Frekvens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most values are concentrated in the lower end of the scale, which might indicate errors in the data, or that towing distance is sually short (with a small number of outliers in other bins, as we can't see these). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for outliers in 'Trekkavstand', by checking towing distance for each top species, and exploring if 'Trekkavstand' affects species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['Hovedart FAO'].isin(top_species)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for species in top_species:\n",
    "    species_data = df_filtered[df_filtered['Hovedart FAO'] == species]\n",
    "    plt.scatter(species_data['Trekkavstand'], [species] * len(species_data), alpha=0.5, label=species)\n",
    "plt.xlabel('Trekkavstand')\n",
    "plt.ylabel('Hovedart FAO')\n",
    "plt.title('Scatterplot av Trekkavstand for de 20 mest fiskede artene')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot confirms the expected outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trekkavstand_sort = df['Trekkavstand'].sort_values(ascending=True)\n",
    "len(trekkavstand_sort)\n",
    "print(trekkavstand_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trekk_list = trekkavstand_sort.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_trekk_value = trekk_list[int(len(trekkavstand_sort)*0.95)] \n",
    "upper_trekk_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% of the values in 'Trekkavstand' are less than or equal to 42146, which again indicates outliers and allows us to set a threshold below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Trekkavstand'] = df['Trekkavstand'].clip(upper=upper_trekk_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['Trekkavstand'].value_counts()\n",
    "zero_gear_rows = df[df['Trekkavstand'] == 0]\n",
    "unique_gear_types = zero_gear_rows['Redskap FDIR'].unique() # list of unique values in 'Redskap FDIR'\n",
    "\n",
    "print(unique_gear_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear_with_zero_trekkavstand = df[df['Trekkavstand'] == 0]['Redskap FDIR'].value_counts()\n",
    "\n",
    "print(gear_with_zero_trekkavstand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "Over 10,000 of the gears with towing distances of 0 are unspecified gear types. We assume that these might be errors, and therefore opted to modify these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for gear in unique_gear_types: \n",
    "    # calculate mean (excluding 0) \n",
    "    mean_value = df[(df['Redskap FDIR'] == gear) & (df['Trekkavstand'] != 0)]['Trekkavstand'].mean()\n",
    "    if pd.notna(mean_value):\n",
    "        # replace 0 with mean value \n",
    "        df.loc[(df['Redskap FDIR'] == gear) & (df['Trekkavstand'] == 0), 'Trekkavstand'] = mean_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Trekkavstand'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Lager et histogram av Trekkavstand med Seaborn\n",
    "sns.histplot(df['Trekkavstand'], bins=50, kde=False)  # kde=False fjerner estimert tetthetskurve\n",
    "plt.title('Histogram av Trekkavstand')\n",
    "plt.xlabel('Trekkavstand')\n",
    "plt.ylabel('Frekvens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows a more evenly distribution of the bins. \n",
    "With more time we would look more into the bin with the highest numbers and what we could do with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['Hovedart FAO'].isin(top_species)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for species in top_species:\n",
    "    species_data = df_filtered[df_filtered['Hovedart FAO'] == species]\n",
    "    plt.scatter(species_data['Trekkavstand'], [species] * len(species_data), alpha=0.5, label=species)\n",
    "plt.xlabel('Trekkavstand')\n",
    "plt.ylabel('Hovedart FAO')\n",
    "plt.title('Scatterplot av Trekkavstand for de 20 mest fiskede artene')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot shows a more distict pattern, however there are still too many samples for this kind of plotting. For instance, it looks like Kolmule typically has a long towing distance. It is not possible to make assumptions about themore frequently caught species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['Hovedart FAO'].isin(top_species)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for species in top_species:\n",
    "    species_data = df_filtered[df_filtered['Hovedart FAO'] == species]['Trekkavstand']\n",
    "    median = species_data.median()\n",
    "    std = species_data.std()\n",
    "    typical_range = species_data[(species_data >= median-std) & (species_data <= median+std)]\n",
    "    \n",
    "    # Plot a line for the median value\n",
    "    plt.hlines(y=species, xmin=median-std, xmax=median+std, color='grey', alpha=0.5)\n",
    "    \n",
    "    # Scatter plot for points within one standard deviation of the median\n",
    "    plt.scatter(typical_range, [species] * len(typical_range), alpha=0.5, label=species)\n",
    "\n",
    "plt.xlabel('Trekkavstand')\n",
    "plt.ylabel('Hovedart FAO')\n",
    "plt.title('Scatterplot av Trekkavstand for de 20 mest fiskede artene')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Move the legend out of the plot\n",
    "plt.tight_layout()  # Adjust layout to accommodate the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatterplot makes it much easier to say something about the towing distance for all species. \n",
    "- The assumtion about Kolmule was correct\n",
    "- The largest fish groups Torsk, Sei, and Hyse all mainly have a short towing distances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Startposisjon bredde', 'Startposisjon lengde', 'Stopposisjon bredde', 'Stopposisjon lengde']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Startposisjon bredde' and 'Startposisjon lengde' have very similar values. Same goes for length. \n",
    "We will therefor later drop both stop positions, and keep both start positions, to reduce features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average latitude and longitude\n",
    "Gives a more nuanced input for the models without sacrificing important information. Reduces some noise. In addition, models that are trained on generalized features, tend to generalize better on new data. May also reduce the training time for the models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups all the unique values from 'Hovedområde start' togethe\n",
    "grouped_start_bredde = df.groupby('Hovedområde start')['Startposisjon bredde']\n",
    "grouped_start_lengde = df.groupby('Hovedområde start')['Startposisjon lengde']\n",
    "grouped_stopp_bredde = df.groupby('Hovedområde start')['Stopposisjon bredde']\n",
    "grouped_stopp_lengde = df.groupby('Hovedområde start')['Stopposisjon lengde']\n",
    "\n",
    "# calculate sum of latitude and longtude of each of the unique main areas. \n",
    "grouped_lengde_count = grouped_start_lengde.count() + grouped_stopp_lengde.count()\n",
    "grouped_lengde_sum = grouped_start_lengde.sum() + grouped_stopp_lengde.sum()\n",
    "grouped_bredde_count = grouped_start_bredde.count() + grouped_stopp_bredde.count()\n",
    "grouped_bredde_sum = grouped_start_bredde.sum() + grouped_stopp_bredde.sum()\n",
    "# calculates the average latitude and logitude. \n",
    "average_lengde = grouped_lengde_sum/grouped_lengde_count\n",
    "average_bredde = grouped_bredde_sum/grouped_bredde_count\n",
    "\n",
    "print(average_bredde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirms that each unique area in 'Hovedområde FAO' has only one, average coordinate assigned to it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering average latitude and longitude \n",
    "\n",
    "We convert average_bredde and average_lengde into numpy arrays, and then stack them vertically (vstack), and transposing them to get the data format we need for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array(average_bredde.values)\n",
    "arr2 = np.array(average_lengde.values)\n",
    "# transpose flips the horisontal and vertical values of the matrix \n",
    "out_arr = np.vstack((arr2, arr1)).transpose()\n",
    "print(out_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(46, 2): 46 unique areas ('Hovedområde start'), each with two features ('average_bredde', 'average_lengde'). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As there are 46 unique areas, we wish to cluster these based on proximity. Therefor we use KMeans clustering. \n",
    "- 11 clusters: we tested with more/fewer clusters, however, we concluded the clusters to be too specific or broad then.\n",
    "- Random_state: 0 for same output each time.\n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.k_means.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters= 11, random_state=0)\n",
    "# fitting model \n",
    "km.fit(out_arr)\n",
    "# cluster labels are retrieved for each data point in out_arr \n",
    "clustered_groups = km.labels_\n",
    "print(clustered_groups)\n",
    "print(len(clustered_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47 unique areas now has a cluster label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customcmap = ListedColormap(['crimson', 'mediumblue', 'darkmagenta', 'lightgreen', 'yellow', 'pink', 'indigo', 'brown', 'black', 'lightblue', 'orange'])\n",
    "fig, ax = plt.subplots(figsize =(8, 8))\n",
    "# Each point's size is set to 100, the colour corresponds to the cluster label, and the color is chosen from the 'custommap' variable\n",
    "plt.scatter(x = average_bredde, y = average_lengde, s = 100, c = clustered_groups, cmap = customcmap)\n",
    "ax.set_xlabel(r'average_bredde', fontsize = 14)\n",
    "ax.set_ylabel(r'average_lengde', fontsize = 14)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points that have the same colour are in the same cluster, and these are in quite close proximity to each other. \n",
    "\n",
    "Source: https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.ListedColormap.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding Location Data \n",
    "\n",
    "Below we are going to place each sample from the dataset into one of the 11 clusters made above, and one hot encode the clusters. \n",
    "Transofrming categorical data into binary format is more suitable for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(main_area, groupnum_label): # group numbers \n",
    "     # Finds the index of the given area in the list 'average_bredde.index' \n",
    "     index_område = list(average_bredde.index).index(main_area)\n",
    "     group_no = groupnum_label[index_område]\n",
    "     # returns the correspondig label from the list of labels.\n",
    "     return group_no\n",
    "\n",
    "def encoded_location(cluster_code, group_location): # one hot encode clusters \n",
    "     # compares a clustere code (0 to 10) with the clustered group labels (also 0 to 10)\n",
    "     if cluster_code == group_location:  # If cluster code = clustered_groups label match the sample belongs to this cluster\n",
    "\n",
    "          return 1 \n",
    "     else: \n",
    "          return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FEATURE ENGINEERING FUNCTION\n",
    "\n",
    "The code applies a series of alterations to the DataFrame. We chose to compile the implementation of all new features in one function, as it makes the code neater. \n",
    "\n",
    "Merging the two 'Bruttotonnasje' features into one single feature simplifies analysis by reducing complexity and managing correlated variables more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame: \n",
    "    df = df.copy() # copy the DataFrame to avoid modifying the original dataset directly. \n",
    "    df['Bruttotonnasje'] = np.maximum(df['Bruttotonnasje 1969'], df['Bruttotonnasje annen']) # Merging 'Bruttotonnasje 1969' and 'Bruttotonnasje annen' into a single feature\n",
    "    df['Temp_location'] = df['Hovedområde start'].apply(location, groupnum_label = clustered_groups) # temporary location for one hot encoding \n",
    "    for clustered_group_no in range(0, 11): # loop through group labels \n",
    "        group_location = 'group_location' + str(clustered_group_no)\n",
    "        df[group_location] = df['Temp_location'].apply(encoded_location, group_location=clustered_group_no) # apply hot_location to one-hot encode the locations\n",
    "    # create 5 new features using vessel_group\n",
    "    df['Redskap top'] = df['Redskap FDIR'].apply(vessel_group, group=1)\n",
    "    df['Redskap bottom'] = df['Redskap FDIR'].apply(vessel_group, group=2)\n",
    "    df['Redskap settegarn'] = df['Redskap FDIR'].apply(vessel_group, group=3)\n",
    "    df['Redskap udefinert garn'] = df['Redskap FDIR'].apply(vessel_group, group=4)\n",
    "    df['Redksap udefinert trål'] = df['Redskap FDIR'].apply(vessel_group, group=5)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering(df)\n",
    "df = df.drop(['Havdybde start', 'Havdybde stopp', 'Season', 'Startdato', 'Temp_location', 'Hovedområde start', 'Lokasjon start (kode)', 'Stopposisjon bredde', 'Stopposisjon lengde', 'Redskap FDIR', 'Bruttotonnasje 1969', 'Bruttotonnasje annen'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # confirming feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LABEL ENGINEERING \n",
    "\n",
    "Since our models are going to predict fish species, 'Hovedart FAO' is our label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Preprocessing 'Hovedart FAO' \n",
    "\n",
    "One-hot encoding label into binary labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fish_group(df_fish, top_fish): \n",
    "    if df_fish == top_fish: \n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "train_labels = pd.DataFrame() # create a new dataframe that will be filled up with all the species within the top_fish\n",
    "\n",
    "# loop through each fish species in top_species to fill up trainlabels with relevant samples, and apply fish_group to onehotencode. \n",
    "for fish in top_species: \n",
    "    train_labels[fish] = df['Hovedart FAO'].apply(fish_group, top_fish=fish)\n",
    "\n",
    "print(df['Hovedart FAO'].value_counts().shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Unsupervised model label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating labels for use in the unsupervised model before dropping 'Hovedart FAO'. feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_of_species_unsup = species_counts[12:]\n",
    "species_to_remove_unsup = rest_of_species_unsup.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_12 = species_counts[:12]\n",
    "top_12_species = top_12.index.tolist()\n",
    "top_12_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_df = df.copy()\n",
    "unsupervised_df = unsupervised_df.loc[df['Hovedart FAO'].isin(species_to_remove_unsup) == False] # Removing fish species not equal to the 12 top species\n",
    "train_labels_unsupervised = unsupervised_df['Hovedart FAO'] # creating a label based on the 12 fish species "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Hovedart FAO', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. NORMALIZATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "feature_to_scale = df[['Month', 'Bruttotonnasje', 'Havdybde avg', 'Trekkavstand', 'Startposisjon bredde', 'Startposisjon lengde']]\n",
    "feature_scaled = scaler.fit_transform(feature_to_scale) # using StandardScaler to scale features \n",
    "df[['Month', 'Bruttotonnasje', 'Havdybde  avg', 'Trekkavstand', 'Startposisjon bredde', 'Startposisjon lengde']] = feature_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirms successful scaling. Values look reasonably distributed within the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DataFrame into PyTorch tensors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor(df.values.astype(np.float32))\n",
    "y_data = torch.tensor(train_labels.values, dtype=torch.float32)\n",
    "\n",
    "type(x_data), type(y_data) # type() to confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Splitting data into training and validation sets\n",
    "Tensordataset: wraps tensors into a dataset for indexed access to paired data.\n",
    "Dataloader: uses dataset and loads data in batches. Beneficial for large datasets. \n",
    "\n",
    "https://www.appsilon.com/post/pytorch-neural-network-tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size = 0.25, shuffle=True)\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "validation_data = TensorDataset(x_valid, y_valid)\n",
    "\n",
    "batch_size = 5000\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_data, batch_size=len(validation_data.tensors[0])) \n",
    "\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_valid:', x_valid.shape)\n",
    "print('y_valid:', y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPERVISED MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RANDOM FOREST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose this model because it is generally effective for complex datasets. Given our features' non-linear relationships, it'll hopefully fit our task. \n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier \n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfc.fit(x_train.numpy(), y_train.numpy().argmax(axis=1))\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = rfc.predict(x_valid.numpy())\n",
    "\n",
    "# Calculate the overall accuracy\n",
    "overall_accuracy = accuracy_score(y_valid.numpy().argmax(axis=1), y_pred)\n",
    "print(f'Overall Accuracy: {overall_accuracy}')\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(y_valid.numpy().argmax(axis=1), y_pred, target_names=top_species, zero_division=1)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is quite high likely due its ability to capture complex relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NAIVE BAYES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose Naive Bayes due to its simplicity and efficiency. \n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Convert y_train and y_valid to class labels\n",
    "y_train_labels = np.argmax(y_train.numpy(), axis=1)\n",
    "y_valid_labels = np.argmax(y_valid.numpy(), axis=1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gnb.fit(x_train.numpy(), y_train_labels)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = gnb.predict(x_valid.numpy())\n",
    "\n",
    "# Calculate the overall accuracy\n",
    "overall_accuracy = accuracy_score(y_valid_labels, y_pred)\n",
    "print(f'Overall Accuracy: {overall_accuracy}')\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(y_valid_labels, y_pred, target_names=top_species, zero_division=1)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's accuracy is quite low. Upon investigating, this isn't surprising. The model's simplicity seemingly hasn't captured complex patterns present in the dataset, and might perform poorly when the data distribution deviates from Gaussian assumption. These are some weaknesses with Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DEEP LEARNING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Neural Network Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FishSpecies defines a neural network for classifying the species. \n",
    "__init__ sets up layers, Relu and dropout. \n",
    "forward function passes through the layers. \n",
    "\n",
    "Source: https://www.nickersonj.com/posts/pytorch-tabular/, https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/\n",
    "\n",
    "\n",
    "ReLU activation function transforms negative numbers to 0, and keeps positive numbers unchanged. \n",
    "\n",
    "Source: https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/\n",
    "\n",
    "\n",
    "Dropout is used to prevent overfitting. 15% of the nodes in the neural network are removed temporarily during the training of the model. \n",
    "\n",
    "Source: https://machinelearningmastery.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FishSpecies(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Creating 5 neuron layers \n",
    "        self.linear1 = nn.Linear(27, 64) \n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.linear3 = nn.Linear(128, 96)\n",
    "        self.linear4 = nn.Linear(96, 32)\n",
    "        self.linear5 = nn.Linear(32, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.linear1(x) # 21 features as input going to 64 nodes in the next layer. \n",
    "        z = self.relu(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.linear2(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.linear3(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.linear4(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.linear5(z)\n",
    "    \n",
    "        return z # Returns one value per class (from the label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FishSpecies()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Learning rate, Loss function, and Optimizer\n",
    "\n",
    "Cross-entropy and ADAM are commonly used in multiclass classification. \n",
    "Well-suited for dealing with probabilities that sum to 1 (softmax)\n",
    "\n",
    "Sources: https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1, \n",
    "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "\n",
    "\n",
    "After trying out different values, we decided to set the learning rate to 0.002, as it balances learning speed and risk of overshooting the loss function's minimum. We chose Cross-entropy for its suitability for mulit-class problems, and used the ADAM optimizer for its adaptive learning rate, enhancing performance with our large dataset. \n",
    "\n",
    "\n",
    "Source: https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing package before training  \n",
    "Used to look at the logits (raw data) before and after Softmax, comparing with labels from y_train, and checking Argmax function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x_train)\n",
    "print('logits:', logits[:5]) \n",
    "loss = loss_fn(logits, y_train)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "optim = softmax(logits[:1])\n",
    "print(optim)\n",
    "print(torch.sum(optim,dim=0))\n",
    "\n",
    "y_pred = torch.argmax(logits[:5], dim=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Function \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred): \n",
    "    acc = accuracy_score(y_true, y_pred) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Training \n",
    "\n",
    "Source: https://www.nickersonj.com/posts/pytorch-tabular/\n",
    "\n",
    "Source batch, epoch: https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "\n",
    "Source tqdm: https://adamoudad.github.io/posts/progress_bar_with_tqdm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "epoch_count, train_loss_values, valid_loss_values = [], [], [] # keeps tracks of epoch number, training loss values and validation loss values for the output plotting\n",
    "\n",
    "\n",
    "for epoch in range(epochs): # trains model by repeatedly going through the data, while updating the model to improve prediction\n",
    "    # Keeping track of loss and accuracy values of all batches in all epochs \n",
    "    epoch_loss = []\n",
    "    epoch_acc = [] \n",
    "    model.train() # Puts the model in training mode \n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\") as bar: # plotting bars\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for x_batch, y_batch in bar: \n",
    "            y_logits_batch = model(x_batch) # forward pass to get the predictions the model thinks is correct based on a forward pass through all samples. \n",
    "            loss = loss_fn(y_logits_batch, y_batch) # compute the loss \n",
    "            \n",
    "            optimizer.zero_grad() # resets the gradients so they don't accumulate each iteration\n",
    "            loss.backward() # backpropagates the prediction loss\n",
    "            optimizer.step() # gradient descent: updates the weights  \n",
    "           \n",
    "            y_pred = torch.argmax(y_logits_batch, dim=1) # \n",
    "            y_train_indices = torch.argmax(y_batch, dim=1)\n",
    "            acc = accuracy_fn(y_train_indices, y_pred) # calculate accuracy \n",
    "            epoch_loss.append(float(loss))\n",
    "            epoch_acc.append(float(acc))\n",
    "            bar.set_postfix(\n",
    "                loss=float(loss),\n",
    "                acc=float(acc)\n",
    "            )\n",
    "\n",
    "    # evaluation mode for the validation data\n",
    "    model.eval() \n",
    "     \n",
    "    with torch.inference_mode(): # ensures that model parameters are not updated (resets for validation) and disables gradient calculations and dropout. \n",
    "\n",
    "        valid_logits = model(x_valid).squeeze() \n",
    "        valid_pred = torch.argmax(valid_logits, dim=1)    \n",
    "        valid_loss = loss_fn(valid_logits, y_valid)\n",
    "        y_valid_indices = torch.argmax(y_valid, dim=1)\n",
    "        valid_acc = accuracy_fn(y_valid_indices, valid_pred) \n",
    "\n",
    "    # for printing progress \n",
    "    if epoch % int(epochs / 20) == 0:\n",
    "        print(f'Epoch: {epoch:4.0f} | Train Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Validation Loss: {valid_loss:.5f}, Accuracy: {valid_acc:.2f}%')\n",
    "        epoch_count.append(epoch)\n",
    "        train_loss_values.append(loss.detach().numpy())\n",
    "        valid_loss_values.append(valid_loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Througout the epochs the validation loss has a lower value than the train loss. This might indicate that the model is generalizing instead of memorizing the training data. The Validation Accuracy is higher than the training accuracy, which is an indicator that the model has good generalization and does not overtrain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accuracy = accuracy_score(y_train_indices, y_pred)\n",
    "print(f'Overall Accuracy: {overall_accuracy}')\n",
    "\n",
    "cr = classification_report(y_train_indices, y_pred, target_names=top_species)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision measures accurcay of positive predictions. Dvptvannsreke, Sild and Snøkrabbe are species with high scores. I.e. Lysing and Kolmule has a lower value indicating that the model does not find as good patterns here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion Supervised\n",
    "\n",
    "To conclude we feel happy with how far we got. The feature engineering could have been done very differently (i.e., cropping differently, more/less groups for gear and location, different features).The features we tested with but decided to not to keep are worth mentioning. For instance, we cropped 'Varighet' to 440min, and tried with day and year (difficult to say what works best as the dataset primarily consists of dates in 2018.), but it didn't affect the accuracy.  \n",
    "More in-depth plotting could also have made the analysis more thorough and helped the feature engineering. \n",
    "\n",
    "When it comes to the supervised methods we quickly saw that the Random Forest had a higher accuracy than Naïve Bayes. This was likely due to its ability to capture complex relationships and effectively handle the features in our dataset. We should have used multiple decision trees with optimal parameters from grid search to improve the accuracy. Naïve Bayes received a quite low accuracy, but this was as expected, due to the size of the dataset. The model might have been negatively impacted when the data distribution didn't align with the Gaussian assumption. With the deep learning method we would have liked to test further with neuronlayers, batchsizes, and epochs, and other activation functions and dropouts, but we feel that the feature engineering was somewhat suitable for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSUPERVISED MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure and some of the code is inspired by: https://www.kaggle.com/code/samuelcortinhas/intro-to-pca-t-sne-umap. \n",
    "\n",
    "Will therefore use PCA, t-SNE and UMAP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for columns to keep for the unsupervised model. Will remove all one hot encoded features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary of the original species with their enlabeled numbers. \n",
    "This will later be used to analyze the clusters to see if there are any patterns regarding the fish species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unsupervised = 20000 # specifying the amount of samples to use \n",
    "\n",
    "# Labelencoding the fish species to numbers\n",
    "le = preprocessing.LabelEncoder() \n",
    "train_labels_unsupervised = le.fit_transform(train_labels_unsupervised[:num_unsupervised])\n",
    "\n",
    "# creating a dict that maps the numbers from label encoding to their corresponding fish species\n",
    "le_name_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "le_name_mapping \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by this source: \n",
    "https://stackoverflow.com/questions/42196589/any-way-to-get-mappings-of-a-label-encoder-in-python-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dictionary to a list \n",
    "label_fishes = list(le_name_mapping.values())\n",
    "label_fishes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating df to chosen features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_df = unsupervised_df[['Startposisjon bredde', 'Startposisjon lengde', 'Trekkavstand', 'Month', 'Havdybde avg', 'Bruttotonnasje']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the features before clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_scale = unsupervised_df[['Startposisjon bredde', 'Startposisjon lengde', 'Trekkavstand', 'Month', 'Havdybde avg', 'Bruttotonnasje']]\n",
    "feature_scaled = scaler.fit_transform(feature_to_scale)\n",
    "unsupervised_df[['Startposisjon bredde', 'Startposisjon lengde', 'Trekkavstand', 'Month', 'Havdybde avg', 'Bruttotonnasje']] = feature_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming that the data is scaled  \n",
    "unsupervised_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numpy array values to torch tensors  \n",
    "x_data_unsupervised = torch.tensor(unsupervised_df.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Chart all fish \n",
    "Visualize our 20 fish species for later comparison.\n",
    "\n",
    "\n",
    "https://www.geeksforgeeks.org/plot-a-pie-chart-in-python-using-matplotlib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each label\n",
    "label_counts = np.bincount(train_labels_unsupervised[:num_unsupervised])\n",
    "\n",
    "percentages = label_counts / label_counts.sum()\n",
    "\n",
    "threshold = 0.02\n",
    "\n",
    "# Filter species names based on the threshold\n",
    "species_names = [le_name_mapping[i] for i in range(len(label_counts)) if percentages[i] >= threshold]\n",
    "filtered_label_counts = [count for count in label_counts if count / label_counts.sum() >= threshold]\n",
    "\n",
    "# Now plot the pie chart with filtered labels and counts\n",
    "plt.figure()\n",
    "plt.pie(filtered_label_counts, labels=species_names, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Distribution of Fish Species\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: Torsk, Dypvannsreke, Sei and Hyse are the only species over 2%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA, 2 dimentions \n",
    "pca = PCA(n_components=2) \n",
    "\n",
    "# Transform xdata to dataframe using pca and reduce to two dimensions\n",
    "X_pca = pca.fit_transform(x_data_unsupervised[:num_unsupervised])\n",
    "\n",
    "# new dataframe using pca values in two columns\n",
    "principal_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2']) \n",
    "\n",
    "# Shape and preview\n",
    "print(principal_df.shape)\n",
    "principal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], s=40)\n",
    "plt.title('PCA plot in 2D')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding colors to each point, one color for each fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spec generates 12 different colors\n",
    "def spec(N):                                             \n",
    "        t = np.linspace(-510, 510, N)\n",
    "        alpha = np.full(N, 255, dtype=np.uint8)                                              \n",
    "        colors =  np.round(np.clip(np.stack([-t, 510-np.abs(t), t, alpha], axis=1), 0, 255)).astype(np.uint8)\n",
    "        return colors / 255.0\n",
    "    \n",
    "colors_rgb_pre = spec(12)\n",
    "print(colors_rgb_pre)\n",
    "colors_rgb = [colors_rgb_pre[i] for i in train_labels_unsupervised] # list containing rgb colors for each sample in label\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], c=colors_rgb)\n",
    "\n",
    "legend_labels = [le_name_mapping[i] for i in range(12) if i in le_name_mapping]\n",
    "\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=label, markersize=10,\n",
    "                        markerfacecolor=colors_rgb_pre[i])\n",
    "                for i, label in le_name_mapping.items()]\n",
    "ax.legend(handles=legend_handles, labels=legend_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/50980810/how-to-create-a-discrete-rgb-colourmap-with-n-colours-using-numpy\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/76226587/how-to-add-a-custom-legend-along-with-default-legends\n",
    "\n",
    "Note: \n",
    "Some global structures, will hopefully become clearer when clustering. \n",
    "For instance, Torsk, Dypvannsreke, Hyse and Sei seem to be somewhat gathered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMEANS for PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 clusters because the dataset of 20,000 samples has 11 species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca = KMeans(n_clusters=11, n_init=15, max_iter=500, random_state=0) \n",
    "\n",
    "# Train and make predictions\n",
    "clusters_pca = kmeans_pca.fit_predict(x_data_unsupervised[:num_unsupervised])\n",
    "\n",
    "# Cluster centers\n",
    "centroids = kmeans_pca.cluster_centers_\n",
    "centroids_pca = pca.transform(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_pca[:30] # trenge rikke i modell 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a matrix with the zeros function. Consists of 20 columns (num of fish species) and 11 rows (num of clusters)\n",
    "arr_cluster = np.zeros([11, 12], dtype = int)\n",
    "arr_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/numpy-zeros-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a loop to fill in the zeros array. \n",
    "Will create 11 lists (one for each cluster) with the amount of each species in each list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_unsupervised): \n",
    "    cluster_number_vertical = clusters_pca[i] # uses the cluster_pca for which number downwards to add 1 \n",
    "    cluster_number_horisontal = train_labels_unsupervised[i] # uses the train_labels_unsupervised to find out which fish to add to horistonally. \n",
    "    arr_cluster[cluster_number_vertical, cluster_number_horisontal] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the array to get the percentage of each fish instead. \n",
    "Will be used for pie charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of each fish species in each cluster\n",
    "arr_percentage = arr_cluster/arr_cluster.sum(axis =1)[:, np.newaxis]\n",
    "print(arr_percentage)\n",
    "\n",
    "percentages = arr_percentage[0, :] # the first cluster \n",
    "labels = label_fishes\n",
    "threshold = 0.001 \n",
    "\n",
    "filtered_percentages = [p for p in percentages if p >= threshold] # filters percentage by threshold\n",
    "filtered_labels = [l for p, l in zip(percentages, labels) if p >= threshold] # filters fish by threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Chart 1 \n",
    "Plotting Pie Charts for visualization of the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.pie(filtered_percentages, labels = filtered_labels, autopct = '%1.1f%%')\n",
    "plt.title(\"Pie chart\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Chart 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_percentage = arr_cluster/arr_cluster.sum(axis =1)[:, np.newaxis]\n",
    "percentages = arr_percentage[4, :]\n",
    "labels = label_fishes\n",
    "threshold = 0.001\n",
    "\n",
    "filtered_percentages = [p for p in percentages if p >= threshold] \n",
    "filtered_labels = [l for p, l in zip(percentages, labels) if p >= threshold] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.pie(filtered_percentages, labels = filtered_labels, autopct = '%1.1f%%')\n",
    "plt.title(\"Pie chart\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Chart 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_percentage = arr_cluster/arr_cluster.sum(axis =1)[:, np.newaxis]\n",
    "percentages = arr_percentage[8, :]\n",
    "labels = label_fishes\n",
    "threshold = 0.001\n",
    "\n",
    "filtered_percentages = [p for p in percentages if p >= threshold] \n",
    "filtered_labels = [l for p, l in zip(percentages, labels) if p >= threshold] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.pie(filtered_percentages, labels = filtered_labels, autopct = '%1.1f%%')\n",
    "plt.title(\"Pie chart\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pie chart notes: \n",
    "\n",
    "We chose to make three random pie charts from the clustering after KMeans. All of them show one dominant species. \n",
    "\n",
    "We see from the first chart (before KMeans) how the top species are divided\n",
    "All pie charts we created after KMeans show one dominant species. The magnitude of them varies, however, the dominant species in all three are within top 5 most frequently caught. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA plot 2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], c=clusters_pca, cmap=\"brg\", s=40)\n",
    "plt.scatter(x=centroids_pca[:,0], y=centroids_pca[:,1], marker=\"x\", s=500, linewidths=3, color=\"black\")\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('PCA plot in 2D')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "We see about 9 clearly flocked clusteres. Distinct centers dividing the cluster can signify that KMeans has found sepeartions in the data. \n",
    "When comparing the clusters to the plot we see that the bundles from the PCA plot are similar to what we see here. \n",
    "There are some outliers and overlapping. This would be interesting to study further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_var = PCA()\n",
    "pca_var.fit(x_data_unsupervised[:num_unsupervised])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "xi = np.arange(1, 1+x_data_unsupervised[:num_unsupervised].shape[1], step=1)\n",
    "yi = np.cumsum(pca_var.explained_variance_ratio_)\n",
    "plt.plot(xi, yi, marker='o', linestyle='--', color='b')\n",
    "\n",
    "# Aesthetics\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, 1+x_data_unsupervised[:num_unsupervised].shape[1], step=1))\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('Explained variance by each component')\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "plt.gca().xaxis.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "With only two components the model represents almost 80% of the variance. \n",
    "This will increase by adding 1 component (approx. 85%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(x_data_unsupervised[:num_unsupervised])\n",
    "\n",
    "# Convert to data frame\n",
    "tsne_df = pd.DataFrame(data = X_tsne, columns = ['tsne comp. 1', 'tsne comp. 2'])\n",
    "\n",
    "\n",
    "# Shape and preview\n",
    "print(tsne_df.shape)\n",
    "tsne_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply the same techinque that we used on PCA to compare but without pie charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spec generates 20 different colors \n",
    "def spec(N):                                             \n",
    "        t = np.linspace(-510, 510, N)\n",
    "        alpha = np.full(N, 255, dtype=np.uint8)                                              \n",
    "        colors =  np.round(np.clip(np.stack([-t, 510-np.abs(t), t, alpha], axis=1), 0, 255)).astype(np.uint8)\n",
    "        return colors / 255.0\n",
    "    \n",
    "colors_rgb_pre = spec(12)\n",
    "print(colors_rgb_pre)\n",
    "colors_rgb = [colors_rgb_pre[i] for i in train_labels_unsupervised] # list containing rgb colors for each sample in label\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tsne_df.iloc[:,0], tsne_df.iloc[:,1], c=colors_rgb)\n",
    "\n",
    "legend_labels = [le_name_mapping[i] for i in range(12) if i in le_name_mapping]\n",
    "\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=label, markersize=10,\n",
    "                        markerfacecolor=colors_rgb_pre[i])\n",
    "                for i, label in le_name_mapping.items()]\n",
    "ax.legend(handles=legend_handles, labels=legend_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans\n",
    "kmeans_tsne = KMeans(n_clusters=11, n_init=15, max_iter=500, random_state=0)\n",
    "\n",
    "# Train and make predictions\n",
    "clusters_tsne = kmeans_tsne.fit_predict(x_data_unsupervised[:num_unsupervised])\n",
    "\n",
    "# Cluster centers\n",
    "centroids = kmeans_pca.cluster_centers_\n",
    "centroids_tsne = pca.transform(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(tsne_df.iloc[:,0], tsne_df.iloc[:,1], c=clusters_tsne, cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('t-SNE plot in 2D')\n",
    "plt.xlabel('tsne component 1')\n",
    "plt.ylabel('tsne  2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "There are clear clusters here, with what seems to be less overlapping than with the PCA. \n",
    "Would be interesting to find out why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "um = umap.UMAP()\n",
    "X_fit = um.fit(x_data_unsupervised[:num_unsupervised])           \n",
    "X_umap = um.transform(x_data_unsupervised[:num_unsupervised])\n",
    "\n",
    "# Convert to data frame\n",
    "umap_df = pd.DataFrame(data = X_umap, columns = ['umap comp. 1', 'umap comp. 2'])\n",
    "\n",
    "# Shape and preview\n",
    "print(umap_df.shape)\n",
    "umap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "um = umap.UMAP()\n",
    "X_fit = um.fit(x_data_unsupervised[:num_unsupervised]) \n",
    "X_umap = um.transform(x_data_unsupervised[:num_unsupervised])\n",
    "\n",
    "# Convert to data frame\n",
    "umap_df = pd.DataFrame(data = X_umap, columns = ['umap comp. 1', 'umap comp. 2'])\n",
    "\n",
    "# Shape and preview\n",
    "print(umap_df.shape)\n",
    "umap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(umap_df.iloc[:,0], umap_df.iloc[:,1], c=clusters_pca, cmap=\"brg\", s=40)\n",
    "\n",
    "# Centroids\n",
    "centroids_umap = um.transform(centroids)\n",
    "plt.scatter(x=centroids_umap[:,0], y=centroids_umap[:,1], marker=\"x\", s=500, linewidths=3, color=\"black\")\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('UMAP plot in 2D')\n",
    "plt.xlabel('umap component 1')\n",
    "plt.ylabel('umap component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "The initial clustering looks less clustered than the others so we chose to focus on PCA and t-SNE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclucion Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think that the clustering methods found some patterns that could be used for understanding the relationships between features and fish species. This will have to be looked further into. With more time, we would find it interesteing to try to use more/less features, especially fishing gear. \n",
    "We would like to try to change the amount of clusters, give more/less data, and look at tha charts again, perhaps using the elbow method. The plotting could be enhanced to retrieve a clearer pattern for analysis. Also looking into normalizing the data differently, silhoutte clustering, or other techniques. \n",
    "We would especially like to explore t-SNE and UMAP more as these work quite differently than the PCA model. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
